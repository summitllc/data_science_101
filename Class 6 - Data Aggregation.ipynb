{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Enrichment\n",
    "\n",
    "Data enrichment and data aggregation are the processes involved around joining merging datasets, creating new columns, calculating values on certain windows, grouping into bins, or even changing the values. All of these processes will assist in the data analysis by providing specific insight into the data. Things such as severity of anomalous data, rolling averages, cumulative sums, or quantities grouped by ages can all make the data easier to interpret. We'll focus on the following for this class:\n",
    "- Querying and Merging dataframes\n",
    "- Aggregating Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying data frames\n",
    "In previous sections we talked about how we learned about how to filter a data frame by building some sort of mapping that would indicate exactly which rows to keep, and which to remove. Another approach is to use the `query` method. \n",
    "\n",
    "Below we import a more expanded weather data set that we've seen in previous lessons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas/master/ch_04/data/nyc_weather_2018.csv')\n",
    "weather"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take note of the `datatype` column. Previously, we had seen only temperature related data, however, now we see a few more options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.datatype.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We could use the same method discussed earlier to create a map or filter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_data_mask = weather[\n",
    "    (weather.datatype == 'SNOW') & (weather.value > 0)\n",
    "]\n",
    "snow_data_mask.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we do have a method called `query` which allows us to write our filters in a format closer to that commonly seen in SQL or even python boolean logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "snow_data = weather.query('datatype == \"SNOW\" and value > 0')\n",
    "snow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even demonstrate that both of these methods return the exact same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "snow_data_mask.equals(snow_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The method that you choose largely depends on preference. Apparently there are some potential speed differences complexity differences that could be encountered, however, that is well beyond the scope of this course. \n",
    "\n",
    "## Merging DataFrames\n",
    "\n",
    "Another crucial part of data analysis is combining datasets together, creating a more complete understanding of the data. There are two types of merges that we typically talk about. Using the vernacular common in from SQL: \n",
    "- `join`: A join combines columns of one dataset to the columns of another dataset based on some condition on the row. If the condition is meet, the columns from dataset B are attached to dataset A. \n",
    "- `union`: A Union can be thought of as appending two data sets that share the same columns.\n",
    "\n",
    "Join is the more complex between the two, so we'll mostly talk about that.\n",
    "\n",
    "### Joining Datasets\n",
    "So far in this class, we have only worked with a single dataset. Joins provide us the ability to take two separate tables or dataframes with related information, and combine them into a single table. For the weather data we've been using, we might perform a join to attach a physical location to the weather measurements using the weather station's id to gain a better idea of how the weather patterns are distributed.\n",
    "\n",
    "The most common types of joins are described below using venn diagrams.  \n",
    "\n",
    "![joins diagram](Assets/joins.jpg \"Joins Diagram\")\n",
    "\n",
    "Think of the circles as the complete set of rows for each dataframe and the shaded region as the rows that are returned as a result of the join. Described briefly, \n",
    "- Inner joins return only the rows that are present in both dataframes,  \n",
    "- Left (Right) joins return all of the rows from the left (right) dataframe, leaving all the values as null or missing (depending on your language) for the columns from the right (left) table if there was no matching ID found in that table\n",
    "- Full or Outer joins return all rows from both tables, leaving missing values in the columns on both sides for missing IDs in either table (Think the left and right joins happening at the same time)\n",
    "\n",
    "Since it can be easier to understand something by doing, let's join some information about the weather stations to the rows of observations from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reading in the weather station data\n",
    "stations = pd.read_csv(\"https://raw.githubusercontent.com/stefmolin/Hands-On-Data-Analysis-with-Pandas/master/ch_04/data/weather_stations.csv\")\n",
    "stations.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first thing we need to do to join two dataframes is determine the rows that we can join on. Note the row called `id`. This value is designed as a unique identifier for each row. If we look at the data from the `weather` dataframe we imported above, we could find that the column `station` contains data that seems to match `stations.id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to determine how to join, we can take a look at the shape of each dataframe. It is also specifically important to check the number of unique values of the column we want to join on. We can check that below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Weather: {weather.shape}\")\n",
    "print(f\"Stations: {stations.shape}\")\n",
    "print(f\"Unique stations from Weather: {weather.station.unique().shape}\")\n",
    "print(f\"Unique stations from Stations: {stations.id.unique().shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we can see that `weather` has more rows than `stations`. In order to preserve the actual data, we want to make sure that we are merging the information from `weather` to `stations`. This is due to the way relation databases store their information, which unfortunately is beyond the scope of this course. We do see that we will lose data on some of the stations (note that `stations.id` has more values than `weather.station`), however this is acceptable since our primary focus is the weather data, not the station data. We could probably go a step further and double check that all of station IDs in `weather` are also present in `stations`, but I'm not that concerned about that right now. \n",
    "\n",
    "All of the joins in we are interested in this course can be executed using the `merge` method and specifying keyword arguments, however there are other methods that could be called instead.\n",
    "\n",
    "Inner join is the default join type of `merge`. The most important argument is the second dataframe to be merged. On top of that you may need to specify the names of the columns to be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inner_joined_tables = weather.merge(stations, left_on='station', right_on='id')\n",
    "inner_joined_tables.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that now we have all of the weather data from before (datatype, date, value, etc.) on the left side of the table, and all of the location data on the right (latitude, longitude, and elevation). Unfortunitely we do have two columns with identical informaiton, but this could be solved easiy by dropping one of the columns after the fact or by renaming the one column beforehand. If the joining columns have the same name, python only includes one column with that name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather.merge(stations.rename({'id':'station'}, axis='columns').drop('elevation', axis='columns'), on='station').head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that with this inner join, there are no missing values on either side of the table (at least none that isn't just bad or missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inner_joined_tables.query('id.isna() or station.isna()').shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is where an inner join differs from right, left, and outer joins. With these types of joins, we allow for the possibility of missing data. Let's attempt a left join. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_joined_tables = weather.merge(stations, left_on='station', right_on='id', how='left')\n",
    "left_joined_tables.query(\"id.isna() or station.isna()\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, we don't have any missing data `station`. This tells us that all station ids found in `weather` were part of the `stations` dataframe. However, remeber that `stations` has more unique values in its `id` column than `weather.station`. If we perform a right join, we know for sure that we will get some missing weather data, because `weather.station` does not have all the values that `stations.id` has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "right_joined_tables = weather.merge(stations, left_on='station', right_on='id', how='right')\n",
    "right_joined_tables.query(\"id.isna() or station.isna()\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice now that we have a set of rows for which the weather station data is present, but under the weather data, we have a bunch of missing values. This indicates to us that the some of the weather stations present in the `stations` dataframe are *not* present in the `weather` dataframe. Doing a quick bit of math, we can confirm that the missing rows matches up with what we expected. Performing an outer join is essentially the same as a left and a right join together, with the above behavior expected for both the left and right tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stations.id.unique()) - len(weather.station.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between an outer join and a right or left join is that we are essentially doing both a left and a right join at the same time. We would get all rows where `weather.station` had a matching value in `stations.id`, all rows where `weather.station` had a value that was not present in `station.id` (in this case that number is 0), and all rows where `station.id` has a value not present in `weather.station`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aggregation\n",
    "Another important function of data analysis is the idea of aggregation. This referes to the process of taking the data and rolling it up into a single value or set of values instead of looking at the individual measurements. We might want the summed, average, or maximum value of the dataset. We can use fairly simple function calls that we have acctually used to a degree before. I will use pivot tables to combine the different aggregations together in our weather data example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather.date = pd.to_datetime(weather.date)\n",
    "weather_pivot = weather.set_index(['date', 'station'])\\\n",
    "    .pivot(columns='datatype', values='value')[['PRCP', 'SNOW', 'TAVG', 'TMAX', 'TMIN']]\n",
    "weather_pivot.query('station == \"GHCND:USW00094728\"')[['PRCP', 'SNOW']].sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the above example, we filter pivot the dataframe so that each datatype is its own column, then we can filter on values from only one specific station, and only select the columns where it makes sense for the aggregation we are performing. In doing this, we have computed a value that reports the total amount of rain and snow fall for the entire measurement period. \n",
    "\n",
    "The above gives us the totals over the entire dataset. We could make this perform the same logic to calculate things like the average per day, or find the days with the most or the least. The method calls are fairly intuitive, and can be quickly found in Panda's documentation. A functionality very commonly used with these aggregation methods is that of grouping (called by `groupby`) to partition the data and perform the aggregation function on each group.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather_pivot.groupby(\"station\")[['PRCP', 'SNOW']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could find the same in or first example by grouping, then filtering at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_pivot.groupby(\"station\")[['PRCP', 'SNOW']].sum().query('station == \"GHCND:USW00094728\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Instead of grouping by the station, we could instead group by the date to find daily statistics such as the minimum of each column on every day across all stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather_pivot.groupby('date').min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Additionally, you can group by multiple columns. It will create a set of nested groups that can be aggregated together for further separation of the metrics being calculated. \n",
    "\n",
    "There is also a pandas class called `Grouper` that seems to allow for even more complex grouping functionality, but we will not go into that class in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "Window functions are a very interesting and useful function that can provide some insight. Sometimes, we want to know the maximum or the average of an entire column, but it can be just as interesting to know the rolling average of some sort of data. Window functions allow us to perform calculations on a group of rows that are close to each other in some way. Using the weather data, we can calculate the average rainfall over the past week. Using the method `rolling`, we can include a predefined set of values in our calculation. In this example, we compute the rolling average of rainfall over the last 7 days at a specific station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall = weather.query(\"datatype == 'PRCP' and station == 'GHCND:USW00094728'\")\\\n",
    "    .set_index(\"date\")\\\n",
    "    .assign(rolling_average=lambda x: x.value.rolling(7).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first few rows do not have a value, as rolling does not have enough rows to pull from at this point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('python-class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e752113259d940eb6cdb8266b4e01ccb99aaf19dbc554f03106f5f3ed78ae61f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
